# Definicion Terraform VPC, GKE, VM & Psql

#### Resoruces
  * google_compute_network
  * google_compute_subnetwork
  * google_container_cluster
  * google_container_node_pool
  * google_compute_instance
  * google_sql_database_instance
  * google_sql_database



##### Orden de Creaci√≥n T√≠pico: 
  * VPC/Subred  
      * -> Cloud SQL Peering/Instancia PSQL 
          * -> Cluster GKE 
              * -> VM.

  + Garant√≠a de Convergencia: 
      + Llama a las APIs de GCP para crear los recursos. Espera activamente a que cada recurso se encuentre en un estado funcional antes de pasar al siguiente. Por ejemplo, no intentar√° crear los nodos de GKE hasta que el plano de control (el Master) del cluster est√© completamente disponible.

  + Actualizaci√≥n del Estado: 
      + Una vez que un recurso se ha creado con √©xito en GCP, Terraform guarda su identidad, configuraci√≥n y atributos en el archivo de estado (terraform.tfstate). Este archivo es cr√≠tico para la gesti√≥n futura de la infraestructura.



A. Definici√≥n de Red (VPC)
    Recurso Clave: __google_compute_network (VPC)__ y __google_compute_subnetwork (Subred)__.

    Teor√≠a: Se define primero una red VPC y al menos una subred dentro de ella. Esta red ser√° el entorno de conectividad para todos los dem√°s recursos.

    Dependencias: Ninguna (es la base).


B. Creaci√≥n del Cluster GKE
    Recurso Clave: __google_container_cluster__ y __google_container_node_pool__.

    Teor√≠a: El cluster de GKE se define para operar dentro de la VPC y la subred creadas. Los nodos de GKE (que son VMs) deben ser creados en esa subred espec√≠fica para comunicarse con la red del proyecto.

    Dependencias: El cluster de GKE depende de la existencia previa de la VPC y la Subred. Se referencian expl√≠citamente en la configuraci√≥n de GKE.

C. Creaci√≥n de la VM (Servidor de Aplicaciones)
    Recurso Clave: __google_compute_instance__.

    Teor√≠a: La VM se define para actuar como un servidor de aplicaciones o cualquier componente auxiliar. Al igual que GKE, debe ser configurada para usar la VPC y la Subred correctas para asegurar la conectividad interna.

    Dependencias: La VM depende de la existencia previa de la VPC y la Subred.

D. Creaci√≥n de la Instancia de PostgreSQL (Cloud SQL)
    Recurso Clave: __google_sql_database_instance__ y __google_sql_database__.

    Teor√≠a: Cloud SQL es un servicio gestionado, por lo que no reside directamente en una subred de la VPC como las VMs. En su lugar, usa un mecanismo de Acceso a Servicio Privado (Private Service Access - PSA) o VPC Peering.

    Para que la VM y el cluster GKE puedan acceder a la base de datos de forma privada, Terraform debe configurar el peering de la VPC entre tu red y la red interna de servicios de Google.

Dependencias:

    La instancia de Cloud SQL depende de la VPC para establecer el peering.

    La base de datos (l√≥gica) depende de la instancia (f√≠sica).













-- 


Conectividad Interna
Una vez creados, el funcionamiento de la conectividad se garantiza porque todos los componentes de la aplicaci√≥n (GKE y VM) residen en la misma VPC, y tienen una ruta privada para comunicarse con la base de datos PSQL (Cloud SQL) a trav√©s del VPC Peering.






***



# Practica 13


* 
* Horizontal Pod Autoscaler
  * DEMO: GKE Horizontal Autoscaler




El Horizontal Pod Autoscaler (HPA) de Kubernetes es un controlador que ajusta autom√°ticamente el n√∫mero de r√©plicas de Pods en un Deployment, ReplicaSet o StatefulSet en funci√≥n de la carga de trabajo observada, como la utilizaci√≥n de CPU o m√©tricas personalizadas.


El HPA opera en un bucle de control continuo:

Monitorizaci√≥n de M√©tricas: El HPA consulta peri√≥dicamente las m√©tricas de uso de los Pods objetivo (por defecto, cada 15-30 segundos). Las m√©tricas m√°s comunes son:

Uso de CPU: Expresado como un porcentaje de los recursos solicitados por el Pod.

Uso de Memoria: Similar al CPU.

M√©tricas Personalizadas y Externas: Como el n√∫mero de solicitudes por segundo (QPS) o m√©tricas de Google Cloud Monitoring (en GKE).

C√°lculo de R√©plicas Deseadas: El HPA compara el valor promedio de la m√©trica observada con el umbral objetivo que t√∫ has definido. Luego, utiliza un algoritmo para calcular el n√∫mero de r√©plicas necesarias para alcanzar ese objetivo.

F√≥rmula B√°sica (para CPU/Memoria):
$$R√©plicas\ Deseadas = \lceil \frac{Uso\ Actual\ Promedio}{Umbral\ Objetivo} \times R√©plicas\ Actuales \rceil$$

Si se configuran m√∫ltiples m√©tricas, el HPA eval√∫a cada una por separado y elige la escala m√°s grande (el mayor n√∫mero de r√©plicas deseadas) para asegurar que ninguna m√©trica exceda su umbral.

Ajuste de Escala (Scaling):
    
    Scale Out (Aumentar): Si la utilizaci√≥n actual es significativamente mayor que el objetivo, el HPA aumenta el n√∫mero de r√©plicas hasta el maxReplicas configurado.
    
    Scale In (Reducir): Si la utilizaci√≥n actual es significativamente menor que el objetivo y ha permanecido as√≠ durante un per√≠odo de estabilizaci√≥n (por defecto, 5 minutos), el HPA reduce el n√∫mero de r√©plicas hasta el minReplicas configurado.



Diferencia Clave con Cluster Autoscaler
Es importante notar que el HPA solo escala los Pods dentro de los nodos existentes. Si el scale out del HPA resulta en Pods que no pueden ser programados por falta de recursos en el nodo, entra en juego el Cluster Autoscaler (otro componente de GKE) para a√±adir nuevos nodos al cl√∫ster para que los Pods puedan ser alojados.








El HPA es el mecanismo de escalado a nivel de aplicaci√≥n m√°s fundamental y poderoso en GKE.

1. Fundamentos del HPA en GKEEl HPA es un controlador del plano de control de Kubernetes que ajusta autom√°ticamente el n√∫mero de r√©plicas de un Deployment, ReplicaSet o StatefulSet para igualar la demanda de carga.2El Bucle de ControlEl controlador HPA opera en un ciclo continuo, revisando peri√≥dicamente (cada 15 segundos por defecto):3Obtener M√©tricas: Consulta el Metrics Server (o fuentes externas) para obtener las m√©tricas de uso de los Pods objetivo.Calcular R√©plicas: Compara el valor actual promedio de la m√©trica con el objetivo (target) que t√∫ definiste.4F√≥rmula del Algoritmo:$$R√©plicas\ Deseadas = \lceil \frac{M√©trica\ Actual\ Promedio}{Umbral\ Objetivo} \times R√©plicas\ Actuales \rceil$$Aplicar Escala: Si el resultado del c√°lculo est√° fuera del rango de tolerancia del 10% (configurable en versiones recientes de Kubernetes), env√≠a un comando al Deployment o StatefulSet para ajustar el n√∫mero de r√©plicas.Requisito Cr√≠tico: Resource RequestsPara que el HPA funcione correctamente con m√©tricas de CPU y Memoria (basadas en porcentaje), es obligatorio que el Pod defina resources.requests para esos recursos.Si estableces un objetivo de CPU del 50%, el HPA lo interpreta como: "Mantener el uso promedio de CPU de los Pods al 50% de lo que cada Pod solicit√≥ (request)."‚öôÔ∏è 2. Tipos de M√©tricas y Fuentes en GKEUna de las grandes fortalezas del HPA es su capacidad para escalar bas√°ndose en m√∫ltiples tipos de m√©tricas.5A. M√©tricas Basadas en Recursos (Resource Metrics)Son las m√°s comunes y se basan en el uso directo de CPU y Memoria de los Pods.M√©tricaTipoTargetOrigen en GKECPUaverageUtilization (Porcentaje)Ejemplo: 60Metrics ServerMemoriaaverageUtilization (Porcentaje)Ejemplo: 75Metrics ServerB. M√©tricas Personalizadas (Custom Metrics)Son m√©tricas espec√≠ficas de la aplicaci√≥n, reportadas desde dentro del cl√∫ster.6Ejemplo: Escalar bas√°ndose en el n√∫mero de mensajes en una cola interna.7Implementaci√≥n en GKE: Generalmente se utiliza el Servicio Administrado para Prometheus de Google Cloud (anteriormente Stackdriver).8 Expones las m√©tricas con un Prometheus Exporter y el adaptador de Prometheus las hace accesibles al HPA.9C. M√©tricas Externas (External Metrics)Son m√©tricas que provienen de servicios de Google Cloud o sistemas completamente externos al cl√∫ster de Kubernetes, que no est√°n directamente asociadas con los Pods escalados.10Ejemplo: Escalar bas√°ndose en:Tama√±o de la cola de Pub/Sub: Si la cola crece por encima de 1000 mensajes pendientes, aumenta los Pods de procesamiento.Latencia de un balanceador de carga (L7) de GCP.Implementaci√≥n en GKE: Se utiliza el Stackdriver Adapter (o el nuevo con Prometheus) para tomar m√©tricas de Cloud Monitoring y exponerlas al HPA.11Importante: Cuando se usan m√∫ltiples m√©tricas (Recurso + Personalizada/Externa), el HPA calcula el n√∫mero de r√©plicas deseado para cada m√©trica y siempre elige el n√∫mero m√°s grande (Scale Out m√°ximo) para garantizar que la carga se maneje.‚öñÔ∏è 3. HPA vs. Otros Escaladores de GKEEs fundamental entender que el HPA es solo una capa en el ecosistema de escalado de GKE.Mecanismo de EscaladoObjeto que Escala¬øEn qu√© se basa?Prop√≥sitoHPA (Horizontal Pod Autoscaler)Pods (R√©plicas de la app)Uso de CPU/Memoria, M√©tricas Personalizadas/Externas.Manejar el tr√°fico de la aplicaci√≥n y la carga de trabajo.VPA (Vertical Pod Autoscaler)Recursos del Pod (CPU/Memoria)Uso hist√≥rico/actual de recursos.Optimizar las requests y limits del Pod para aumentar la eficiencia (No se recomienda usar VPA y HPA en la misma m√©trica de recursos).Cluster Autoscaler (CA)Nodos (M√°quinas VM)Hay Pods pendientes de ser programados (No hay recursos en los nodos).Ajustar la infraestructura de GKE a la demanda del Pod.12Flujo de Escalado Completo en GKE:El tr√°fico aumenta.El uso de CPU de los Pods supera el 60% (Target del HPA).HPA ordena crear 5 Pods m√°s (Scale Out).El Scheduler de Kubernetes no encuentra espacio para 3 de los nuevos Pods $\rightarrow$ Pasan a estado Pending.El Cluster Autoscaler detecta los Pods pendientes y provisiona un nuevo nodo VM.13Los Pods pendientes se programan en el nuevo nodo $\rightarrow$ El sistema est√° escalado de forma segura y rentable.üõ°Ô∏è 4. Comportamiento y Estabilizaci√≥n (Configurable Scaling)El HPA tiene mecanismos internos para prevenir el "Thrashing" (escalado repetitivo e inestable, sube-baja-sube-baja).14Tolerancia: Por defecto, el HPA solo act√∫a si la m√©trica promedio excede la meta en m√°s del 10% (configurable en Kubernetes 1.33+).Tiempo de Estabilizaci√≥n (Stabilization Window):Scale Down (Reducci√≥n): Despu√©s de un evento de Scale Out, el HPA debe esperar un per√≠odo de tiempo (Scale Down Stabilization Window, 5 minutos por defecto) antes de poder reducir las r√©plicas.15 Esto evita que una ca√≠da temporal de la carga (o un pico muy corto) haga que el HPA reduzca los Pods inmediatamente despu√©s de haberlos creado.16Scale Up (Aumento): Despu√©s de un evento de Scale Up, el HPA debe esperar un tiempo m√°s corto (casi instant√°neo) para volver a escalar hacia arriba si la m√©trica sigue alta.Puedes ajustar estos tiempos en la secci√≥n spec.behavior del objeto HPA para optimizar la respuesta a tus patrones de tr√°fico.


***

